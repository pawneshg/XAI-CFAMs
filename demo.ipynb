{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configurations\n",
    "# from model.config import data_dir, weights_load_path\n",
    "\n",
    "# base_dir = \".\"\n",
    "# data_dir = \"../../datasets/coco\"  \n",
    "# weights_load_path = \"./model/weights/weight_stepLR_2.pth\"\n",
    "# activation_base_dir = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model.resnet18 import resnet18\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from activation.visualize import visualize\n",
    "from activation.utils import predict_label\n",
    "import torch\n",
    "from model.coco_dataset import get_test_coco_dataset_iter\n",
    "from sacred_config import ex\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIMENT_LOCAL_CONFIG = \"./local_config.json\"\n",
    "with open(EXPERIMENT_LOCAL_CONFIG) as f:\n",
    "    json_dict = json.load(f)\n",
    "param = SimpleNamespace(**json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/pawnesh/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(param.pretrain, param.class_ids, param.finetune,param.weights_load_path, param.end_epoch, \n",
    "                 param.start_epoch, param.checkpoints, param.save_weights_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize(images, titles, figsize, ncols, nrows, save_path, normalise=True):\n",
    "#     \"\"\" Visualization of images \"\"\"\n",
    "#     fig, axes = plt.subplots(figsize=figsize, ncols=ncols, nrows=nrows, squeeze=False)\n",
    "#     # if (len(images) != (ncols*nrows)) and (len(titles) != (ncols*nrows)):\n",
    "#     #     return False\n",
    "#     imgs = iter(images)\n",
    "#     titles = iter(titles)\n",
    "#     try:\n",
    "#         for row_ind in range(nrows):\n",
    "#             for col_ind in range(ncols):\n",
    "#                 img = next(imgs)\n",
    "#                 # Todo:\n",
    "#                 if normalise:\n",
    "#                     mean = np.array([0.485, 0.456, 0.406])\n",
    "#                     std = np.array([0.229, 0.224, 0.225])\n",
    "#                     img = std * img + mean\n",
    "#                     img = np.clip(img, 0, 1)\n",
    "#                 axes[row_ind, col_ind].imshow(img)\n",
    "#                 axes[row_ind, col_ind].set_title(next(titles))\n",
    "#     except Exception as err:\n",
    "#         print(err)\n",
    "#     finally:\n",
    "#         fig.savefig(save_path)\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction_visualization(model, class_ids, activation_save_path, num_of_cams=2):\n",
    "    \"\"\" Visualize the activation maps of a prediction model. \"\"\"\n",
    "\n",
    "    len_of_labels = int(len(class_ids))\n",
    "    nrows = 5\n",
    "    start_ind, ind = 0, 0\n",
    "#     data_to_visualize, labels_for_vis_data = get_visualization_data(model, get_coco_samples_per_class)\n",
    "\n",
    "    for ind in range(0, len_of_labels, nrows):\n",
    "        end_ind = start_ind + (nrows*(num_of_cams+1))\n",
    "        print(\"start\", start_ind)\n",
    "        print(\"end\", end_ind)\n",
    "#         is_success = visualize(data_to_visualize[start_ind:end_ind], labels_for_vis_data[start_ind:end_ind],\n",
    "#                                figsize=(10, 15), nrows=nrows, ncols=num_of_cams+1,\n",
    "#                                save_path=f\"{activation_save_path}/activation_map_{ind}.jpg\")\n",
    "        \n",
    "        start_ind += nrows*(num_of_cams+1)\n",
    "\n",
    "#     is_success = visualize(data_to_visualize[start_ind:], labels_for_vis_data[start_ind:], figsize=(10, 15),\n",
    "#                            nrows=nrows, ncols=num_of_cams+1,\n",
    "#                            save_path=f\"{activation_save_path}/activation_map_{ind+1}.jpg\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 0\n",
      "end 15\n",
      "start 15\n",
      "end 30\n",
      "start 30\n",
      "end 45\n",
      "start 45\n",
      "end 60\n",
      "start 60\n",
      "end 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_visualization(model, param.class_ids, param.activation_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_coco_samples_per_class(number_of_classes, samples_per_class=1):\n",
    "    \"\"\" Fetch samples for each class .\n",
    "    # Todo: Extend the functionality to support param: samples_per_class.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    #images = torch.Tensor(number_of_classes, 3, 224, 224)\n",
    "    #labels = torch.Tensor(number_of_classese)\n",
    "    labels = []\n",
    "    test_data_iter = get_test_coco_dataset_iter(param.class_ids, param.val_meta_file, \n",
    "                                                param.val_data_dir, param.batch_size, \n",
    "                                                param.num_workers)##\n",
    "    visited_classes = []\n",
    "    for data_batch, label_batch in test_data_iter:\n",
    "        for data, label in zip(data_batch, label_batch):\n",
    "            label = label.numpy().item()\n",
    "            if label not in visited_classes:\n",
    "                #torch.cat(data, out=images)\n",
    "                images.append(data)\n",
    "                visited_classes.append(label)\n",
    "                labels.append(label)\n",
    "                #torch.cat()\n",
    "            if len(visited_classes) == number_of_classes:\n",
    "                break\n",
    "    return torch.stack(images), torch.Tensor(labels)\n",
    "\n",
    "\n",
    "# def predict_label(model, data, num_of_cams=2):\n",
    "#     \"\"\"predict the label for an image.\"\"\"\n",
    "# #     if isinstance(data, np.ndarray):\n",
    "# #         data = torch.from_numpy(data)\n",
    "# #     data.unsqueeze_(0)\n",
    "#     last_conv_layer = \"layer4\"\n",
    "#     model.eval()\n",
    "#     features_blobs = []\n",
    "\n",
    "#     def last_conv_layer_hook(module, grad_input, grad_output):\n",
    "#         features_blobs.append(grad_output.data.cpu().numpy())\n",
    "\n",
    "#     model._modules.get(last_conv_layer).register_forward_hook(last_conv_layer_hook)\n",
    "#     softmax_weight = np.squeeze(list(model.parameters())[-2].data.numpy())\n",
    "#     #print(softmax_weight)\n",
    "#     result = model(data)\n",
    "#     result = torch.nn.functional.softmax(result, dim=1).data.squeeze()\n",
    "#     result = torch.topk(result, k=2, dim=1)\n",
    "#     return result, features_blobs, softmax_weight\n",
    "\n",
    "\n",
    "def get_visualization_data(model, get_samples_func):\n",
    "    \"\"\" Fetch Visualization data. input_image, cam1, cam2 .\"\"\"\n",
    "    data_to_visualize = []\n",
    "    labels_for_vis_data = []\n",
    "    images, labels = get_samples_func(number_of_classes=24)##\n",
    "    t_topk, features, softmax_weight = predict_label(model, images)\n",
    "    probs, pred_label = t_topk\n",
    "    probs, pred_label = probs.detach().numpy(), pred_label.detach().numpy()\n",
    "    \n",
    "    for each_img, each_label in zip(images.numpy(), labels.numpy()):\n",
    "        \n",
    "        each_img = each_img.numpy().transpose((1, 2, 0))\n",
    "        # input data\n",
    "        data_to_visualize.append(each_img)\n",
    "        labels_for_vis_data.append(each_label)\n",
    "        # cam1 data\n",
    "        data_to_visualize.append(each_img)\n",
    "        labels_for_vis_data.append(result.numpy().item())\n",
    "        # cam2 data\n",
    "        data_to_visualize.append(each_img)\n",
    "        labels_for_vis_data.append(result.numpy().item())\n",
    "\n",
    "    return data_to_visualize, labels_for_vis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-52d66208424d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_samples_per_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-0a00ce862a24>\u001b[0m in \u001b[0;36mget_coco_samples_per_class\u001b[0;34m(number_of_classes, samples_per_class)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                 param.num_workers)##\n\u001b[1;32m     12\u001b[0m     \u001b[0mvisited_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/exp_projs/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "imgs, labs = get_coco_samples_per_class(number_of_classes=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_topk, features = predict_label(model, imgs)\n",
    "probs, pred_label = t_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, pred_label = probs.detach().numpy(), pred_label.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-2][20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = list(model.parameters())[-2][20]\n",
    "t2 = list(model.parameters())[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([198, 404])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(t1*t2, k=2).indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cam = features[0][1][198]\n",
    "cam -= np.min(cam)\n",
    "cam /= np.max(cam)\n",
    "cam_img = np.uint8(255 * cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  8, 12, 18,  0, 21, 19,  2, 20,  4,  4, 11, 22, 13,  0, 10,  6,\n",
       "       21, 17,  6,  7, 16, 23, 14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-24-b40890421333>\u001b[0m(40)\u001b[0;36mpredict_label\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     38 \u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 40 \u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     42 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> result\n",
      "tensor([[-3.4233e-01, -3.4654e+00, -2.0715e+00, -8.7725e-01, -2.3427e-01,\n",
      "          5.3524e+00,  1.7179e+00,  1.3368e+00, -9.7993e-01, -9.8762e-01,\n",
      "         -2.2510e+00, -1.1951e+00, -1.6797e+00, -4.2046e-01,  2.0662e+00,\n",
      "         -1.2579e+00, -5.3066e-01, -1.1895e+00,  2.1218e-03,  5.3471e-01,\n",
      "         -8.1082e-01,  1.4076e+00,  2.6985e+00,  1.2873e+00],\n",
      "        [ 4.5497e+00, -8.1549e-01,  8.3232e-01,  2.0941e+00,  2.0961e-01,\n",
      "          2.2897e+00, -6.9344e-01,  8.9430e-01,  1.1651e+00,  1.8754e-01,\n",
      "         -7.0124e-01, -1.1833e+00, -1.6573e+00,  3.1535e-01, -5.8290e-01,\n",
      "         -9.6892e-01, -2.6252e+00, -2.4093e+00, -1.7131e+00, -2.0215e-01,\n",
      "          1.3241e-01, -4.6746e-01, -2.2333e-01,  4.1607e-01],\n",
      "        [ 9.9945e-01, -1.2253e+00, -1.8894e+00, -1.1318e-01, -7.0166e-01,\n",
      "          1.0051e+00,  1.4650e+00,  3.9867e+00, -2.0890e+00,  2.1309e-01,\n",
      "         -4.1922e+00, -2.7197e+00, -2.6571e+00, -8.4918e-01, -2.1198e-01,\n",
      "         -5.1239e-01, -5.6007e-01, -6.6491e-01,  4.5098e-01,  3.7687e+00,\n",
      "          5.6401e-02,  5.6672e-01,  1.4602e+00,  3.7084e+00],\n",
      "        [-1.0975e+00, -2.2140e+00, -1.0227e+00, -9.6751e-01, -2.6987e-01,\n",
      "         -6.9442e-01,  7.0193e-01,  1.3363e+00, -1.9977e+00, -1.4440e+00,\n",
      "         -1.7057e+00, -1.6459e+00, -1.3633e+00,  2.4526e+00,  1.9186e+00,\n",
      "          7.3782e-01,  4.3127e-02,  1.1156e+00,  1.7456e+00,  6.8830e-01,\n",
      "         -1.2575e+00, -5.5385e-02,  2.4277e+00,  1.7280e+00],\n",
      "        [-1.8875e+00, -1.7225e+00, -1.0748e+00,  1.0406e+00, -1.0645e+00,\n",
      "          5.9209e-01, -3.0913e-01,  2.0534e+00, -2.3106e+00, -1.6969e+00,\n",
      "         -3.5168e+00, -3.4194e+00, -3.4977e+00,  2.9319e-01,  2.8509e-01,\n",
      "         -1.8034e+00, -2.4778e+00, -8.1785e-01, -4.8944e-02,  3.9416e+00,\n",
      "          3.0321e+00,  7.4810e+00,  3.9309e+00,  1.3468e+00],\n",
      "        [-2.6558e-01, -3.0095e+00, -1.2758e-01, -8.8996e-01, -1.5723e+00,\n",
      "          8.2798e-01, -1.7766e+00,  1.4425e+00,  6.8211e-01,  4.0703e-01,\n",
      "         -1.8366e+00, -2.5688e+00, -2.0327e+00,  2.2233e+00,  4.2269e-01,\n",
      "          2.7423e-01, -1.8164e+00,  1.0506e+00,  1.0715e+00,  1.9124e+00,\n",
      "         -4.1291e-02,  1.5349e+00,  1.3500e+00,  2.3711e+00],\n",
      "        [ 1.2562e+00,  2.0498e+00,  3.1809e+00,  4.5974e-01,  6.2935e-02,\n",
      "          1.5763e-02,  4.2860e-01, -1.5482e+00,  3.3172e+00,  4.6297e+00,\n",
      "          1.6403e+00, -7.4668e-01, -2.4174e+00,  2.0260e+00,  3.4758e-02,\n",
      "         -2.1883e+00, -2.4695e+00, -1.9246e+00, -9.4122e-01, -1.6991e+00,\n",
      "         -2.3169e+00, -1.4004e+00, -1.2423e+00, -3.8551e-01],\n",
      "        [-2.1647e-01,  2.1648e+00,  2.3347e-01,  8.9692e-01,  9.2811e-01,\n",
      "          1.3190e+00,  1.2011e+00, -1.0507e+00,  6.6877e-01, -9.4208e-02,\n",
      "         -1.1091e+00, -1.8426e+00, -1.2665e+00,  2.2365e+00, -4.7449e-01,\n",
      "         -1.6294e+00, -2.2555e+00, -7.0090e-01, -1.1825e+00,  3.1186e-01,\n",
      "         -2.0672e+00, -1.4056e+00, -1.8879e+00,  2.5085e-01],\n",
      "        [-1.4209e-01,  7.7607e-01,  3.5039e+00,  1.1583e+01,  1.8400e+00,\n",
      "          1.8036e+00,  3.7394e-01, -1.4138e-01,  2.0161e-01,  1.3497e+00,\n",
      "         -2.0183e+00, -1.8856e+00, -9.6441e-01,  1.1054e+00, -1.2985e+00,\n",
      "         -1.5099e+00, -2.3994e+00, -2.9671e+00, -1.5632e+00, -8.2821e-02,\n",
      "         -2.1767e+00, -9.6633e-01, -1.1893e+00, -1.9196e+00],\n",
      "        [ 2.4343e-01,  2.7901e-01, -3.4046e-01, -3.4538e-01,  1.5418e+00,\n",
      "          9.3313e-01,  5.0523e-01, -8.4308e-01,  7.6685e+00,  2.8861e+00,\n",
      "          2.5482e+00, -6.2760e-01,  4.1625e-01,  1.5981e+00,  1.6045e-01,\n",
      "         -1.3369e+00, -2.3257e+00, -2.1729e+00, -1.3614e+00, -1.7683e+00,\n",
      "         -3.4159e+00, -2.3031e+00, -9.6814e-01, -2.2660e-01],\n",
      "        [-1.1274e+00,  1.3417e+00, -1.4389e-01, -8.4334e-01,  1.0851e+00,\n",
      "          1.1778e-01,  4.4657e+00, -1.5716e-01, -8.5451e-04,  5.1256e-02,\n",
      "         -6.8304e-01,  5.5739e-01,  6.2002e+00,  1.4402e+00, -8.0815e-01,\n",
      "         -2.6511e+00, -1.1664e+00, -4.2640e-01, -1.6106e+00, -1.4074e+00,\n",
      "         -2.0467e+00, -2.3095e+00, -7.5256e-01, -7.4005e-01],\n",
      "        [ 2.3296e-01, -1.6466e+00,  1.6717e+00,  3.1287e-01, -1.5269e-01,\n",
      "          2.4471e+00, -1.5897e-01, -3.2986e-01, -2.5024e-01, -1.5077e+00,\n",
      "         -6.1186e-01, -2.0809e+00, -9.5052e-01,  3.4285e+00,  8.6336e-01,\n",
      "         -1.3447e+00, -1.7785e+00, -2.0986e-01, -1.9877e-01,  6.0737e-02,\n",
      "         -1.6018e+00,  3.0268e-01,  1.5319e+00,  1.0751e+00],\n",
      "        [-1.7446e+00, -3.0148e+00, -2.1181e+00, -4.7461e+00, -1.9257e+00,\n",
      "          7.0460e-01, -1.0140e+00,  5.4693e+00, -3.9420e-01, -1.0609e-01,\n",
      "         -2.1637e+00, -2.6847e+00, -1.4083e+00, -6.8995e-02, -5.8580e-01,\n",
      "         -6.4144e-01, -1.8759e+00, -1.0241e+00, -3.9599e-02,  9.3881e+00,\n",
      "         -8.6903e-01, -3.9400e-02,  2.9165e+00,  6.1121e+00],\n",
      "        [-1.2451e+00, -1.4150e+00, -2.0263e-01, -1.5695e+00, -8.2800e-01,\n",
      "          7.1021e-01, -4.1114e-01, -3.3224e-01,  4.3912e+00,  2.8188e+00,\n",
      "          8.1139e+00,  3.0287e+00,  1.5091e+00,  5.0401e-01, -1.3947e+00,\n",
      "         -2.4161e+00, -2.7509e+00, -2.6223e+00, -1.5513e+00, -2.4548e-01,\n",
      "         -2.4038e+00, -2.5293e+00, -1.2534e+00, -4.5908e-01],\n",
      "        [-8.5216e-01, -1.4769e+00,  1.4513e-01, -5.4305e-01,  1.1248e+00,\n",
      "         -2.6776e-01,  2.9327e+00, -1.0555e+00,  7.2601e-01,  9.1137e-01,\n",
      "         -1.0091e+00,  3.1849e+00,  7.0072e+00,  8.9223e-01, -1.6059e+00,\n",
      "         -2.7054e+00, -2.1324e+00, -5.5482e-01, -1.9173e+00, -7.6251e-01,\n",
      "         -2.2439e+00, -2.1140e+00, -4.5058e-01, -1.5148e+00],\n",
      "        [-2.9343e+00, -2.8105e+00, -3.1441e+00, -2.6928e+00, -1.2158e+00,\n",
      "         -1.5696e+00, -4.2955e-01,  4.3473e-01, -2.0399e+00, -4.5205e-01,\n",
      "         -3.0665e+00, -3.6171e+00, -2.2424e+00, -1.3380e+00,  3.3829e+00,\n",
      "          8.6483e+00,  6.4289e+00,  3.0115e+00,  4.6269e+00, -9.3778e-01,\n",
      "         -1.7593e+00,  1.9668e+00,  6.3370e-02,  1.0340e+00],\n",
      "        [-3.0698e-01, -3.2565e+00, -3.4897e-01, -1.7583e+00, -1.2071e+00,\n",
      "          2.2598e+00,  2.9592e-01,  8.6360e-01,  8.5905e-01, -1.6990e+00,\n",
      "         -2.3530e+00, -2.3643e+00, -1.9978e+00,  1.3684e+00,  5.3676e+00,\n",
      "         -4.0634e-01,  9.8443e-01, -4.0155e-01,  9.6799e-02,  5.1763e-01,\n",
      "         -5.5547e-01,  1.0327e+00,  2.3003e+00,  2.1928e+00],\n",
      "        [-6.8137e-01, -1.7972e+00, -8.9012e-01, -1.5600e+00,  7.4923e-01,\n",
      "         -1.1362e-01, -1.5801e+00, -7.3090e-01, -2.5909e+00, -1.9584e+00,\n",
      "         -2.7501e+00, -2.7985e+00, -3.0015e+00,  3.9380e-01,  8.2828e-01,\n",
      "          2.5445e+00, -9.0144e-01,  3.2040e+00,  2.6691e+00, -6.4382e-02,\n",
      "          9.5996e-01,  1.6363e+00,  3.3360e+00,  1.0213e-01],\n",
      "        [-1.6276e+00, -1.4372e+00, -2.0211e+00, -1.1674e+00, -1.2618e+00,\n",
      "          1.2555e-01,  5.0440e-01, -1.5101e-01, -2.2296e+00, -1.7446e+00,\n",
      "         -2.2008e+00, -3.3927e+00, -1.9739e+00, -2.7174e-01, -9.0536e-01,\n",
      "         -4.6502e-01, -6.6280e-01,  5.2125e-01,  7.7791e-01, -9.0828e-01,\n",
      "          1.0659e+01,  1.7879e+00,  2.2529e+00, -8.7683e-01],\n",
      "        [-5.2466e-01,  6.4111e+00,  3.8270e-01, -3.0295e-01,  1.5485e+00,\n",
      "          7.2332e-01,  3.3234e+00, -7.0331e-01, -5.7314e-02, -1.1580e+00,\n",
      "         -1.8361e+00, -2.0299e+00, -6.1648e-01,  6.2941e-01, -2.9810e-01,\n",
      "         -1.9005e+00, -6.0686e-01, -1.5324e+00, -1.5657e+00, -4.5367e-01,\n",
      "         -2.1682e-01, -2.3198e+00, -1.3444e+00, -3.8824e-01],\n",
      "        [ 1.9068e+00,  2.7948e+00,  4.6037e+00,  2.8519e+00,  1.6288e+00,\n",
      "          1.8144e+00,  1.4180e+00, -4.3926e-01,  5.6716e-01, -1.0691e+00,\n",
      "         -1.1927e+00, -2.7972e+00, -1.5362e+00,  3.4445e+00, -5.3768e-01,\n",
      "         -1.9687e+00, -2.4848e+00, -1.2874e+00, -9.6217e-01, -1.6575e+00,\n",
      "         -1.3365e+00, -4.5421e-01, -9.3450e-01, -1.0617e+00],\n",
      "        [-1.0843e+00, -2.9850e+00, -6.6497e-01, -1.0870e+00, -1.7333e+00,\n",
      "          4.0796e-01, -1.4327e+00,  1.9472e-02, -9.7376e-01, -1.0370e+00,\n",
      "         -1.0269e+00, -2.7788e+00, -2.3182e+00,  9.4422e-01,  2.2050e+00,\n",
      "          2.9939e+00, -5.5370e-01,  2.9996e+00,  2.5681e+00, -5.1339e-01,\n",
      "         -6.6834e-01,  2.0429e+00,  9.5775e-01,  4.7200e-01],\n",
      "        [-1.3458e+00, -2.8098e+00, -2.6758e+00, -3.5275e+00, -1.9439e-01,\n",
      "         -2.3615e+00, -1.5124e+00, -4.4538e-01, -4.0888e+00, -2.4908e+00,\n",
      "         -3.3733e+00, -3.9272e+00, -3.5876e+00, -1.0666e-02,  2.4209e+00,\n",
      "          4.7814e+00,  1.1831e+00,  4.9076e+00,  6.8347e+00, -1.1633e-01,\n",
      "          1.2892e+00,  4.2534e+00,  3.0720e+00,  1.1050e+00],\n",
      "        [-2.1892e+00, -3.4524e-01, -1.9465e+00, -1.0014e+00, -9.3173e-01,\n",
      "          1.0116e+00,  2.8626e+00,  3.4784e-02,  2.3790e+00,  3.1062e+00,\n",
      "          3.9715e+00,  8.1828e+00,  4.3692e+00, -9.2839e-01, -1.6325e+00,\n",
      "         -3.5368e+00, -3.0408e+00, -1.8105e+00, -1.5550e+00, -2.0573e+00,\n",
      "         -3.9854e+00, -3.8282e+00, -7.0055e-01, -2.4516e+00]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-24-b40890421333>\u001b[0m(41)\u001b[0;36mpredict_label\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     39 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     40 \u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 41 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     42 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> result\n",
      "torch.return_types.topk(\n",
      "values=tensor([[ 5.3524,  2.6985],\n",
      "        [ 4.5497,  2.2897],\n",
      "        [ 3.9867,  3.7687],\n",
      "        [ 2.4526,  2.4277],\n",
      "        [ 7.4810,  3.9416],\n",
      "        [ 2.3711,  2.2233],\n",
      "        [ 4.6297,  3.3172],\n",
      "        [ 2.2365,  2.1648],\n",
      "        [11.5831,  3.5039],\n",
      "        [ 7.6685,  2.8861],\n",
      "        [ 6.2002,  4.4657],\n",
      "        [ 3.4285,  2.4471],\n",
      "        [ 9.3881,  6.1121],\n",
      "        [ 8.1139,  4.3912],\n",
      "        [ 7.0072,  3.1849],\n",
      "        [ 8.6483,  6.4289],\n",
      "        [ 5.3676,  2.3003],\n",
      "        [ 3.3360,  3.2040],\n",
      "        [10.6585,  2.2529],\n",
      "        [ 6.4111,  3.3234],\n",
      "        [ 4.6037,  3.4445],\n",
      "        [ 2.9996,  2.9939],\n",
      "        [ 6.8347,  4.9076],\n",
      "        [ 8.1828,  4.3692]], grad_fn=<TopkBackward>),\n",
      "indices=tensor([[ 5, 22],\n",
      "        [ 0,  5],\n",
      "        [ 7, 19],\n",
      "        [13, 22],\n",
      "        [21, 19],\n",
      "        [23, 13],\n",
      "        [ 9,  8],\n",
      "        [13,  1],\n",
      "        [ 3,  2],\n",
      "        [ 8,  9],\n",
      "        [12,  6],\n",
      "        [13,  5],\n",
      "        [19, 23],\n",
      "        [10,  8],\n",
      "        [12, 11],\n",
      "        [15, 16],\n",
      "        [14, 22],\n",
      "        [22, 17],\n",
      "        [20, 22],\n",
      "        [ 1,  6],\n",
      "        [ 2, 13],\n",
      "        [17, 15],\n",
      "        [18, 17],\n",
      "        [11, 12]]))\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a4a8d724a1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b6907b0ba0ce>\u001b[0m in \u001b[0;36mprediction_visualization\u001b[0;34m(model, class_ids, activation_save_path, num_of_cams)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata_to_visualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_for_vis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_visualization_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_coco_samples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_of_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b40890421333>\u001b[0m in \u001b[0;36mget_visualization_data\u001b[0;34m(model, get_samples_func)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0meach_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# main\n",
    "prediction_visualization(model, param.class_ids, param.activation_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
